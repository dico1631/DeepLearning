{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch: Optimization & Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/pytorch/examples/tree/master/mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 1 # shuffle을 아래서 할 것\n",
    "\n",
    "batch_size = 64\n",
    "test_batch_size = 64\n",
    "\n",
    "no_cuda = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = not no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('dataset', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('dataset', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model과 Optimization 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device) # 위의 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parmaneter : __init__의 내용물\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1, 5, 5])\n",
      "torch.Size([20])\n",
      "torch.Size([50, 20, 5, 5])\n",
      "torch.Size([50])\n",
      "torch.Size([500, 800])\n",
      "torch.Size([500])\n",
      "torch.Size([10, 500])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# tensorflow의 summary 대신\n",
    "params = list(model.parameters())\n",
    "for i in range(8):\n",
    "    print(params[i].size())\n",
    "    \n",
    "# conv1의 w, b, conv2의 w, b, fc1의 w, b, fc2의 w, b (순서대로)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 학습하기 전에 Model이 Train할 수 있도록 Train Mode로 변환\n",
    "    - Convolution 또는 Linear 뿐만 아니라, DropOut과 추후에 배우게 될 Batch Normalization과 같이 parameter를 가진 Layer들도 학습하기 위해 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train mode와 evaluation mode가 있음 \n",
    "model.train()  # train mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델에 넣기 위한 첫 Batch 데이터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# image, label\n",
    "data.shape, target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 추출한 Batch 데이터를 cpu 또는 gpu와 같은 device에 compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, target = data.to(device), target.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), torch.Size([64]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- gradients를 clear해서 새로운 최적화 값을 찾기 위해 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer 초기화\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 준비한 데이터를 model에 input으로 넣어 output을 얻음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = prediction\n",
    "output = model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Model에서 예측한 결과를 Loss Function에 넣음\n",
    "    - 여기 예제에서는 Negative Log-Likelihood Loss 라는 Loss Function을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output = y_pred, target = y(label)\n",
    "loss = F.nll_loss(output, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한 세트 (loss 계산 > gradients 계산 > parameter update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Back Propagation을 통해 Gradients를 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 계산된 Gradients는 계산된 걸로 끝이 아니라 Parameter에 Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 최적화 과정을 반복하여 학습 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "epochs = 1\n",
    "log_interval = 100 # log 확인하는 interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.367843\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.321535\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.550576\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.344955\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.282179\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.343969\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.209759\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.231541\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.372720\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.369015\n",
      "\n",
      "Test set: Average Loss: 0.2817, Accuracy: 9208/10000 (92%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 최종본\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    # Train Mode로 변환\n",
    "    model.train()\n",
    "    \n",
    "    # enumerate : idx도 같이 나오는 함수\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device) # cpu\n",
    "        optimizer.zero_grad() # opt 초기화\n",
    "        output = model(data) # data를 model에 넣음 > 예측치 :output\n",
    "        loss = F.nll_loss(output, target) # loss 계산\n",
    "        loss.backward() # gradient 계산\n",
    "        optimizer.step() # w 업데이트\n",
    "        \n",
    "        # 중간에 잘 되고 있는지 log 확인\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset), # len 2개 = 현 data가 몇 번째 data인지, 전체 data의 크기\n",
    "                100 * batch_idx / len(train_loader), loss.item() # 몇 % 돌아갔는지\n",
    "            ))\n",
    "            \n",
    "        # 여기까지가 model 학습 !\n",
    "        \n",
    "    # 여기서부터 평가!                \n",
    "    model.eval()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average Loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞에서 model.train() 모드로 변한 것처럼 평가 할 때는 model.eval()로 설정\n",
    "    - Batch Normalization이나 Drop Out 같은 Layer들을 잠금"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(20, 50, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=800, out_features=500, bias=True)\n",
       "  (fc2): Linear(in_features=500, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval() # evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- autograd engine, 즉 backpropagatin이나 gradient 계산 등을 꺼서 memory usage를 줄이고 속도를 높임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "# no_grad : back prop 안하게 함.\n",
    "with torch.no_grad():\n",
    "    data, target = next(iter(test_loader))\n",
    "    data, target = data.to(device), target.to(device) # cpu\n",
    "    output = model(data) # 모델에 넣어서 pred\n",
    "    \n",
    "    # log를 보기 위해서\n",
    "    # sum : 1개의 스칼라로 변환됨. 누적을 보는 것\n",
    "    # mean과 값 자체를 보는 none도 있음\n",
    "    test_loss += F.nll_loss(output, target, reduction='sum').item() \n",
    "    \n",
    "    # 제일 큰 max 값을 뽑음 > 제일 강한 위치의 idx > classification\n",
    "    # dim=1 : 차원을 그대로 유지\n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    \n",
    "    # eq : 예측과 답이 같은지 아닌지 확인 > T/F로 나옴\n",
    "    # sum : T끼리 F끼리 합침\n",
    "    correct = pred.eq(target.view_as(pred)).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 10])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.shape # dim은 유지, 10개 중에 1개롤 class 선정됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pred와 같은 크기\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5],\n",
       "        [9],\n",
       "        [5],\n",
       "        [6],\n",
       "        [7],\n",
       "        [8],\n",
       "        [6],\n",
       "        [3],\n",
       "        [0],\n",
       "        [2],\n",
       "        [3],\n",
       "        [7],\n",
       "        [9],\n",
       "        [5],\n",
       "        [3],\n",
       "        [3],\n",
       "        [4],\n",
       "        [2],\n",
       "        [8],\n",
       "        [0],\n",
       "        [9],\n",
       "        [7],\n",
       "        [9],\n",
       "        [4],\n",
       "        [8],\n",
       "        [9],\n",
       "        [1],\n",
       "        [5],\n",
       "        [2],\n",
       "        [5],\n",
       "        [0],\n",
       "        [7],\n",
       "        [1],\n",
       "        [5],\n",
       "        [2],\n",
       "        [5],\n",
       "        [1],\n",
       "        [8],\n",
       "        [7],\n",
       "        [7],\n",
       "        [4],\n",
       "        [1],\n",
       "        [5],\n",
       "        [9],\n",
       "        [6],\n",
       "        [4],\n",
       "        [0],\n",
       "        [6],\n",
       "        [8],\n",
       "        [9],\n",
       "        [4],\n",
       "        [5],\n",
       "        [1],\n",
       "        [4],\n",
       "        [1],\n",
       "        [4],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [4],\n",
       "        [1],\n",
       "        [6],\n",
       "        [1]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.view_as(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.view_as(pred).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(54)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 64개 중 54개 맞춘 것\n",
    "pred.eq(target.view_as(pred)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.84375"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.eq(target.view_as(pred)).sum().item() / 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28.772645950317383"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 틀린 것\n",
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 맞춘 것\n",
    "correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전체로 나눠서 비율을 구함\n",
    "test_loss /= len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0028772645950317383"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average Loss: 0.4464, Accuracy: 8886/10000 (89%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # evaluation\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader: # for로 모든 test data를 넣음\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "# test 중에 나온 손실들의 평균 = 이 모델에서 나왔던 손실들의 평균\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('\\nTest set: Average Loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
